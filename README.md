# 🧠 Evalia — The Intelligence Layer for Prompt Evaluation & Security

> **Where prompts meet proof and security meets compliance.** Intelligent platform for evaluating, benchmarking, and securing large language model (LLM) applications — from performance optimization to SOC2 compliance.

Evalia empowers AI engineers, security teams, and compliance officers to build better and safer LLM applications. Evaluate prompt performance across tasks, domains, and models — then validate security with automated penetration testing and compliance checks.

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-00a393?style=flat&logo=fastapi)](https://fastapi.tiangolo.com)
[![Next.js](https://img.shields.io/badge/Next.js-14+-black?style=flat&logo=next.js)](https://nextjs.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5+-3178c6?style=flat&logo=typescript)](https://www.typescriptlang.org)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-412991?style=flat&logo=openai)](https://openai.com)
[![Anthropic](https://img.shields.io/badge/Anthropic-Claude-orange?style=flat)](https://anthropic.com)
[![Docker](https://img.shields.io/badge/Docker-ready-2496ed?style=flat&logo=docker)](https://www.docker.com)
[![Security](https://img.shields.io/badge/Security-OWASP%20LLM%20Top%2010-red?style=flat&logo=owasp)](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
[![Compliance](https://img.shields.io/badge/Compliance-SOC2%20%7C%20ISO27001-green?style=flat&logo=security)](https://www.aicpa.org/soc)

---

## 💡 **Why Evalia?**

Building great AI experiences requires **evidence-driven prompt design** and **robust security validation**. Evalia turns prompt engineering from trial-and-error into a measurable, scientific process — while ensuring your LLM applications meet security standards and compliance requirements before reaching production.

---

## 🌟 **Core Features**

### 🧩 **Prompt Evaluation Engine**

- **Automated A/B Testing** of prompt variants across datasets, models, and criteria
- **Multi-Model Benchmarking** across GPT-4, Claude, Gemini, and more
- **Performance Metrics** including accuracy, tone, relevance, and custom criteria
- **Regression Detection** to catch prompt performance degradation

### 🧠 **LLM-as-Judge Scoring**

- **Advanced Model Evaluation** using GPT-4 or Claude for qualitative scoring
- **Reasoning & Explanations** for why outputs succeed or fail
- **Custom Evaluation Criteria** tailored to your specific use cases
- **Confidence Scoring** and uncertainty quantification

### 🔄 **Intelligent Prompt Optimization**

- **AI-Powered Feedback** — Get actionable suggestions to improve your prompts based on evaluation results
- **Dataset-Specific Optimization** — Tailored recommendations for your specific use cases and data patterns
- **Performance Analysis** — Identify weak points, ambiguities, and missing instructions
- **Iterative Refinement** — Apply suggestions and re-evaluate to measure improvement
- **Best Practice Recommendations** — Learn prompt engineering patterns that work for your domain

### 📝 **Prompt Version Control**

- **Git-like Versioning** — Track every change to your prompts with full history
- **Visual Diff Tool** — Side-by-side comparison of prompt versions with highlighted changes
- **Branching & Merging** — Experiment with prompt variants and merge successful changes
- **Rollback & Restore** — Revert to previous versions when changes don't improve performance
- **Change Annotations** — Document why changes were made and their impact on metrics
- **Collaborative Editing** — Team members can propose, review, and approve prompt changes

### 🏷️ **Domain-Aware Datasets**

- **Business-Specific Datasets** for legal, e-commerce, healthcare, and more
- **Adaptive Dataset Evolution** that improves with feedback
- **Realistic Example Generation** using LLMs when datasets don't exist
- **Version Control** for dataset changes and improvements

### 🔄 **Dynamic Dataset Builder**

- **On-the-Fly Generation** of realistic test cases using LLMs
- **Feedback Integration** to continuously improve dataset quality
- **Domain Expertise** injection for specialized use cases
- **Synthetic Data Pipeline** for privacy-sensitive applications

### 📊 **Insight Dashboard**

- **Performance Visualization** over time with interactive charts
- **Domain Comparison** to see which contexts perform best
- **Model Performance** side-by-side comparisons
- **Cost Analysis** and token usage optimization

### ⚡ **Continuous Evaluation Pipelines**

- **CI/CD Integration** for automatic prompt re-evaluation
- **Webhook Support** for real-time notifications
- **Scheduled Evaluations** for ongoing monitoring
- **Alert System** for performance degradation

### 🔍 **Hybrid Metrics**

- **LLM-Based Evaluation** for nuanced quality assessment
- **Rule-Based Metrics** for objective measurements
- **Human-in-the-Loop** validation and feedback
- **Custom Scoring Functions** for specialized requirements

### 🧬 **Model & Domain Comparison**

- **Cross-Model Benchmarking** (OpenAI, Anthropic, Google, etc.)
- **Domain-Specific Performance** analysis
- **Cost-Performance Trade-offs** visualization
- **Latency vs Quality** optimization insights

### 🛡️ **Security Testing & Penetration**

- **Prompt Injection Detection** — Test for jailbreaks, instruction bypasses, and adversarial prompts
- **Data Exfiltration Prevention** — Detect PII leakage and sensitive information disclosure
- **Adversarial Attack Simulation** — Run OWASP Top 10 for LLMs and custom attack vectors
- **Authorization Testing** — Validate role-based access control and permission boundaries
- **API Endpoint Testing** — Point Evalia at your LLM API with custom authentication
- **Attack Vector Library** — 100+ pre-built security tests based on real-world exploits

### 🔐 **SOC2 Compliance Validation**

- **Automated Compliance Testing** — Validate SOC2 Trust Service Criteria automatically
- **Multi-Framework Support** — SOC2, ISO 27001, GDPR, CCPA, HIPAA compliance
- **Security Controls Auditing** — Verify authentication, authorization, and encryption
- **Compliance Reporting** — Generate audit-ready reports with evidence and remediation
- **Continuous Monitoring** — Track compliance score and identify gaps in real-time
- **Risk Assessment** — CVSS-style vulnerability scoring and risk heat maps

### 🎯 **Multi-Target Testing**

- **Prompt-Based Testing** — Provide system prompts and configurations for testing
- **API Integration** — Test live endpoints with API keys, OAuth, JWT, or custom auth
- **CI/CD Integration** — Block deployments that fail security or compliance thresholds
- **Scheduled Security Scans** — Automated penetration testing on daily/weekly schedules
- **Webhook Alerts** — Real-time notifications for critical vulnerabilities

---

## 🚀 **Use Cases**

### 🎯 **Customer Support Optimization**

Evaluate customer-support prompts across different industries and optimize response quality, tone, and accuracy.

### 📊 **Content Generation Benchmarking**

Compare summarization, QA, and content generation prompts between models (GPT-4, Claude, Gemini) to find the best fit.

### 🔬 **Research & Development**

Fine-tuning validation, RAG system optimization, and prompt experimentation for AI labs and research teams.

### 🏢 **Enterprise AI Tooling**

Internal tooling for prompt experimentation, A/B testing, and performance monitoring at scale.

### 🔒 **LLM Security Auditing**

Test your LLM applications for security vulnerabilities before attackers do — prompt injection, data leakage, and authorization bypasses.

### 📋 **SOC2 & Compliance Certification**

Achieve and maintain SOC2, ISO 27001, and regulatory compliance for your AI products with automated testing and audit-ready reports.

### 🛡️ **Red Team AI Applications**

Continuous adversarial testing and penetration of your LLM APIs to identify weaknesses and ensure secure deployments.

### 🔄 **Iterative Prompt Engineering**

Get AI-powered feedback on your prompts after every evaluation — learn what works, what doesn't, and how to optimize for your specific dataset and use case.

### 📝 **Prompt Version Control & Collaboration**

Track prompt evolution like code with git-style versioning — compare changes, see performance deltas, rollback when needed, and collaborate with your team on prompt improvements.

---

## 🏗️ **Architecture**

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Next.js       │────│   FastAPI       │────│  Evaluation     │
│   Dashboard     │    │   Backend       │    │   Engine        │
│                 │    │                 │    │                 │
│ • React Charts  │    │ • Python 3.11+ │    │ • Multi-Model   │
│ • Security UI   │    │ • Async/Await   │    │ • LLM-as-Judge  │
│ • TypeScript    │    │ • Pydantic      │    │ • Metrics       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
    ┌────────────────────────────┴────────────────────────────┐
    │                                                         │
┌───▼────┐ ┌────────┐ ┌────────┐ ┌─────────────┐ ┌──────────┐
│ Redis  │ │ Celery │ │ Qdrant │ │   Dataset   │ │PostgreSQL│
│ Cache  │ │ Queue  │ │Vector  │ │   Storage   │ │ Database │
└────────┘ └────────┘ └────────┘ └─────────────┘ └──────────┘
                │
    ┌───────────┴───────────┐
    │                       │
┌───▼────────┐    ┌────────▼─────┐
│  Security  │    │  Compliance  │
│  Testing   │    │  Validation  │
│  Engine    │    │  Engine      │
│            │    │              │
│ • Attacks  │    │ • SOC2       │
│ • Scans    │    │ • ISO 27001  │
│ • Reports  │    │ • GDPR       │
└────────────┘    └──────────────┘
```

---

## 🚀 **Quick Start**

### 1. **Prerequisites**

```bash
# Install uv (ultra-fast Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Node.js 18+
# https://nodejs.org/

# Install Docker & Docker Compose
# https://docs.docker.com/get-docker/
```

### 2. **Environment Setup**

```bash
# Copy environment files
cp backend/.env.template backend/.env
cp frontend/.env.template frontend/.env

# Set your API keys in backend/.env
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here  # Optional for Gemini

# Vector database for embeddings (choose one)
QDRANT_URL=http://localhost:6333
# OR
PINECONE_API_KEY=your_pinecone_key_here
```

### 3. **Start with Docker Compose** ⚡

```bash
# Start all services (recommended for first run)
docker-compose up -d

# Or use the development setup
docker-compose -f docker-compose.dev.yml up -d
```

### 4. **Manual Development Setup** 🛠️

```bash
# Backend
cd backend
uv venv
source .venv/bin/activate  # or `.venv\Scripts\activate` on Windows
uv pip install -e .
uv pip list
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Frontend (in another terminal)
cd frontend
npm install
npm run dev
```

### 5. **Access Evalia** 🎉

- **Dashboard**: <http://localhost:3000>
- **Backend API**: <http://localhost:8000>
- **API Docs**: <http://localhost:8000/docs>
- **Evaluation Results**: <http://localhost:3000/evaluations>

---

## 🔧 **Evaluation Configuration**

### **Supported Models**

Evalia supports evaluation across multiple LLM providers:

```python
# OpenAI Models
"gpt-4"                    # Best for complex reasoning
"gpt-4-turbo"             # Faster GPT-4 variant
"gpt-3.5-turbo"           # Cost-effective option

# Anthropic Models
"claude-3-opus"           # Most capable Claude model
"claude-3-sonnet"         # Balanced performance
"claude-3-haiku"          # Fast and efficient

# Google Models
"gemini-pro"              # Google's flagship model
"gemini-pro-vision"       # Multimodal capabilities

# Open Source (via Ollama)
"llama2"                  # Meta's Llama 2
"mistral"                 # Mistral 7B
"codellama"               # Code-specialized model
```

### **Evaluation Types**

**Quality Metrics**

- **Accuracy**: Correctness of responses
- **Relevance**: How well responses address the prompt
- **Coherence**: Logical flow and consistency
- **Completeness**: Coverage of required information

**LLM-as-Judge Evaluation**

- **GPT-4 Judge**: Use GPT-4 to evaluate response quality
- **Claude Judge**: Anthropic's Claude for nuanced assessment
- **Custom Criteria**: Define domain-specific evaluation criteria
- **Confidence Scoring**: Uncertainty quantification

### **Dataset Configuration**

**Built-in Domains**

- **Customer Support**: Service inquiries and responses
- **Content Generation**: Blog posts, summaries, descriptions
- **Code Generation**: Programming tasks and solutions
- **Legal**: Contract analysis and legal reasoning
- **Healthcare**: Medical information and advice
- **E-commerce**: Product descriptions and recommendations

**Custom Datasets**

- **CSV Import**: Upload your own evaluation datasets
- **API Integration**: Connect to existing data sources
- **Synthetic Generation**: AI-generated test cases
- **Version Control**: Track dataset changes over time

### **Prompt Optimization Workflow**

After each evaluation, Evalia's AI analyzes your results and provides actionable feedback:

**1. Performance Analysis**
- Identify which test cases failed and why
- Detect patterns in successful vs unsuccessful responses
- Analyze response quality across different metrics

**2. AI-Generated Suggestions**
- **Missing Instructions**: "Add guidance for handling user complaints"
- **Ambiguous Phrasing**: "Clarify what 'professional tone' means in context X"
- **Edge Cases**: "Your prompt doesn't handle empty inputs or multi-part questions"
- **Examples**: "Add few-shot examples for better consistency"
- **Constraints**: "Define output format and length requirements"

**3. Iterative Refinement**
- Apply suggestions manually or auto-generate optimized version
- Re-run evaluation to measure improvement
- Compare before/after metrics
- Iterate until target performance achieved

**Example Optimization Feedback:**
```json
{
  "current_score": 7.2,
  "target_score": 9.0,
  "gap_analysis": {
    "accuracy": { "current": 85%, "target": 95%, "priority": "high" },
    "tone": { "current": 72%, "target": 90%, "priority": "medium" }
  },
  "top_suggestions": [
    {
      "issue": "30% of responses were too verbose",
      "suggestion": "Add: 'Keep responses concise (2-3 sentences max)'",
      "expected_impact": "+15% on tone metric"
    },
    {
      "issue": "Failed edge case: multi-language queries",
      "suggestion": "Add: 'If query is not in English, politely ask for translation'",
      "expected_impact": "+8% on accuracy metric"
    }
  ],
  "optimized_prompt": "Your improved prompt with all suggestions applied..."
}
```

### **Prompt Version Control**

Track and manage prompt evolution with git-like workflows:

**Version Management**
- Every change creates a new version (semantic versioning: v1.2.3)
- Commit messages explain what changed and why
- Full history preserved for auditing and rollback

**Visual Diff Viewer**
- Side-by-side comparison of any two versions
- Highlighted additions, deletions, and modifications
- Performance delta shows impact of each change
- Character-level and semantic diff options

**Branching & Experimentation**
- Create branches to test risky changes
- Multiple team members can work on different variants
- Merge successful experiments back to main
- Automatic conflict detection and resolution

**Collaborative Workflow**
```
main branch (production)
  v1.0.0 → v1.1.0 → v1.2.0
              ↓
         [branch: experiment-formal-tone]
              v1.1.1 → v1.1.2
                         ↓
                    [evaluate]
                         ↓
                   Performance: +12%
                         ↓
                  [merge to main]
                         ↓
                      v1.3.0 ✨
```

**Example Diff Output:**
```diff
Version: v1.2.0 → v1.3.0
Performance Delta: Accuracy +5.2%, Tone +3.1%, Relevance -0.5%

System Prompt Changes:
  You are a helpful customer support assistant.
- Be casual and friendly.
+ Always respond in a professional and empathetic tone.
+ Keep responses concise (2-3 sentences maximum).

  When a customer has a complaint:
+ 1. Acknowledge their frustration
+ 2. Offer a concrete solution
+ 3. Provide next steps

- If you don't know the answer, make your best guess.
+ If you don't know the answer, say so and offer to escalate.
```

### **Security Testing Coverage**

**OWASP LLM Top 10 Vulnerabilities**

Evalia automatically tests for all OWASP LLM Top 10 security risks:

1. **LLM01: Prompt Injection** — Direct and indirect injection attacks
2. **LLM02: Insecure Output Handling** — XSS, CSRF, and backend exploits via LLM outputs
3. **LLM03: Training Data Poisoning** — Detection of compromised training data
4. **LLM04: Model Denial of Service** — Resource exhaustion and availability attacks
5. **LLM05: Supply Chain Vulnerabilities** — Third-party plugin and model risks
6. **LLM06: Sensitive Information Disclosure** — PII leakage and data exfiltration
7. **LLM07: Insecure Plugin Design** — Authorization and input validation in plugins
8. **LLM08: Excessive Agency** — Privilege escalation and unauthorized actions
9. **LLM09: Overreliance** — Hallucination detection and factual accuracy
10. **LLM10: Model Theft** — Intellectual property protection and model extraction

**Additional Security Tests**

- **Jailbreak Detection**: Advanced prompt engineering to bypass safety guardrails
- **Context Window Attacks**: Token limit exploitation and memory exhaustion
- **Role Confusion**: System prompt leakage and role boundary violations
- **Data Leakage**: PII, credentials, and proprietary information exposure
- **Authorization Bypass**: RBAC and permission boundary testing

---

## 📚 **API Endpoints**

### **Prompt Evaluation**

```bash
# Create a new evaluation
POST /api/v1/evaluations
{
  "name": "Customer Support Prompt v2",
  "prompts": [
    {
      "id": "prompt_a",
      "content": "You are a helpful customer support agent..."
    },
    {
      "id": "prompt_b", 
      "content": "As a customer service representative..."
    }
  ],
  "dataset_id": "customer_support_v1",
  "models": ["gpt-4", "claude-3-sonnet"],
  "evaluation_criteria": ["accuracy", "tone", "helpfulness"]
}

# Get evaluation results
GET /api/v1/evaluations/{evaluation_id}/results

# List all evaluations
GET /api/v1/evaluations

# Compare prompt performance
GET /api/v1/evaluations/{evaluation_id}/compare

# Get AI-powered optimization feedback
GET /api/v1/evaluations/{evaluation_id}/optimize
{
  "feedback": {
    "overall_score": 7.5,
    "strengths": ["Clear instructions", "Good examples"],
    "weaknesses": ["Ambiguous edge cases", "Missing tone guidance"],
    "suggestions": [
      {
        "type": "add_instruction",
        "priority": "high",
        "description": "Add explicit handling for edge cases X and Y",
        "example": "When the user asks about..., respond with..."
      }
    ],
    "optimized_prompt": "Your optimized prompt here..."
  }
}
```

### **Prompt Version Control**

```bash
# Create a new prompt version
POST /api/v1/prompts/{prompt_id}/versions
{
  "content": "Updated prompt content...",
  "commit_message": "Improved clarity and added examples",
  "parent_version_id": "v1.2.3"
}

# Get prompt version history
GET /api/v1/prompts/{prompt_id}/versions

# Compare two versions (diff)
GET /api/v1/prompts/{prompt_id}/diff?from=v1.2.3&to=v1.3.0
{
  "from_version": "v1.2.3",
  "to_version": "v1.3.0",
  "changes": [
    {
      "type": "addition",
      "line": 5,
      "content": "+ Always respond in a professional tone."
    },
    {
      "type": "deletion",
      "line": 8,
      "content": "- Be casual and friendly."
    }
  ],
  "performance_delta": {
    "accuracy": +5.2,
    "tone": +3.1,
    "relevance": -0.5
  }
}

# Rollback to previous version
POST /api/v1/prompts/{prompt_id}/rollback
{
  "target_version": "v1.2.3",
  "reason": "v1.3.0 had worse performance on customer support dataset"
}

# Create a branch for experimentation
POST /api/v1/prompts/{prompt_id}/branches
{
  "branch_name": "experiment-formal-tone",
  "base_version": "v1.2.3"
}

# Merge branch back to main
POST /api/v1/prompts/{prompt_id}/merge
{
  "source_branch": "experiment-formal-tone",
  "target_branch": "main",
  "strategy": "auto"
}
```

### **Dataset Management**

```bash
# Upload a new dataset
POST /api/v1/datasets
{
  "name": "Customer Support Q&A",
  "domain": "customer_support",
  "data": [
    {
      "input": "How do I return a product?",
      "expected_output": "You can return products within 30 days...",
      "metadata": {"category": "returns"}
    }
  ]
}

# Get dataset details
GET /api/v1/datasets/{dataset_id}

# Generate synthetic dataset
POST /api/v1/datasets/generate
{
  "domain": "e_commerce",
  "size": 100,
  "criteria": "product_descriptions"
}

# List available datasets
GET /api/v1/datasets
```

### **Model Management**

```bash
# Get available models
GET /api/v1/models

# Test model connectivity
POST /api/v1/models/{model_id}/test
{
  "prompt": "Hello, world!"
}

# Get model performance metrics
GET /api/v1/models/{model_id}/metrics

# Compare models
POST /api/v1/models/compare
{
  "models": ["gpt-4", "claude-3-sonnet"],
  "prompt": "Explain quantum computing",
  "evaluation_criteria": ["accuracy", "clarity"]
}
```

### **Analytics & Insights**

```bash
# Get evaluation analytics
GET /api/v1/analytics/evaluations/{evaluation_id}

# Performance trends over time
GET /api/v1/analytics/trends?timeframe=30d

# Cost analysis
GET /api/v1/analytics/costs?model=gpt-4&timeframe=7d

# Domain performance comparison
GET /api/v1/analytics/domains/compare
```

### **Background Tasks**

```bash
# Run evaluation asynchronously
POST /api/v1/tasks/evaluations
{
  "evaluation_id": "eval_123",
  "priority": "high"
}

# Get task status
GET /api/v1/tasks/{task_id}

# Schedule recurring evaluation
POST /api/v1/tasks/schedule
{
  "evaluation_id": "eval_123",
  "schedule": "0 9 * * 1"  # Every Monday at 9 AM
}
```

### **Security Testing**

```bash
# Create a new security scan (prompt-based)
POST /api/v1/security/scans
{
  "name": "ChatBot Security Audit",
  "target_type": "prompt",
  "system_prompt": "You are a helpful assistant...",
  "model": "gpt-4",
  "attack_vectors": ["prompt_injection", "data_exfiltration", "jailbreak"],
  "severity_threshold": "medium"
}

# Create security scan (API-based)
POST /api/v1/security/scans
{
  "name": "Production API Security Test",
  "target_type": "api",
  "endpoint_url": "https://api.example.com/chat",
  "auth_type": "bearer",
  "auth_credentials": {"token": "your_api_key"},
  "attack_vectors": ["all"],
  "compliance_frameworks": ["soc2", "owasp_llm_top10"]
}

# Get scan results
GET /api/v1/security/scans/{scan_id}/results

# List all vulnerabilities
GET /api/v1/security/vulnerabilities?severity=critical&status=open

# Get attack vector library
GET /api/v1/security/attack-vectors

# Run specific attack test
POST /api/v1/security/test-attack
{
  "attack_vector_id": "prompt_injection_001",
  "target": "You are a helpful assistant...",
  "model": "gpt-4"
}
```

### **Compliance Validation**

```bash
# Run SOC2 compliance check
POST /api/v1/compliance/validate
{
  "framework": "soc2_type2",
  "target_type": "api",
  "endpoint_url": "https://api.example.com/chat",
  "auth_type": "api_key",
  "auth_credentials": {"api_key": "your_key"}
}

# Get compliance score
GET /api/v1/compliance/score?framework=soc2

# Generate compliance report
POST /api/v1/compliance/reports
{
  "framework": "soc2_type2",
  "scan_ids": ["scan_123", "scan_456"],
  "format": "pdf"
}

# List compliance frameworks
GET /api/v1/compliance/frameworks

# Get control validation results
GET /api/v1/compliance/controls?framework=soc2&status=failing
```

---

## 🔧 **Configuration**

### **Backend Settings** (`backend/.env`)

```bash
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here

# Default Evaluation Settings
DEFAULT_EVALUATION_MODEL=gpt-4
DEFAULT_JUDGE_MODEL=gpt-4
ENABLE_COST_TRACKING=true
MAX_CONCURRENT_EVALUATIONS=5

# Vector Database (choose one)
VECTOR_DATABASE=qdrant
QDRANT_URL=http://localhost:6333
# OR
VECTOR_DATABASE=pinecone
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=gcp-starter

# Database & Cache
DATABASE_URL=postgresql://user:pass@localhost:5432/evalia
REDIS_URL=redis://localhost:6379/0

# Background Tasks
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/1
ENABLE_ASYNC_EVALUATIONS=true

# Monitoring & Analytics
ENABLE_METRICS=true
PROMETHEUS_PORT=9090
LOG_LEVEL=INFO

# Security & Compliance
ENABLE_SECURITY_TESTING=true
ENABLE_COMPLIANCE_VALIDATION=true
OWASP_LLM_TOP10_ENABLED=true
DEFAULT_ATTACK_SEVERITY_THRESHOLD=medium
MAX_CONCURRENT_SECURITY_SCANS=3
SECURITY_SCAN_TIMEOUT=300

# Compliance Frameworks
SOC2_VALIDATION_ENABLED=true
ISO27001_VALIDATION_ENABLED=false
GDPR_CHECKS_ENABLED=true
HIPAA_CHECKS_ENABLED=false

# Prompt Optimization
ENABLE_PROMPT_OPTIMIZATION=true
OPTIMIZATION_MODEL=gpt-4  # Model used for generating optimization suggestions
AUTO_SUGGEST_ON_EVALUATION=true
MIN_EVALUATION_SCORE_FOR_SUGGESTIONS=6.0

# Prompt Version Control
ENABLE_VERSION_CONTROL=true
DEFAULT_BRANCH=main
AUTO_VERSION_ON_SAVE=true
SEMANTIC_VERSIONING=true
MAX_VERSION_HISTORY=100
```

### **Frontend Settings** (`frontend/.env.local`)

```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_APP_NAME=Evalia
NEXT_PUBLIC_ENABLE_ANALYTICS=true
```

---

## 🚢 **Deployment**

### **Using Docker** (Recommended)

```bash
# Production build
docker-compose -f docker-compose.prod.yml up -d

# Or use the deployment script
./backend/scripts/deploy.sh
```

### **Manual Deployment**

```bash
# Backend
cd backend
uv pip install -e .
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker

# Start Celery workers for evaluation tasks
celery -A app.core.celery_app:celery_app worker --queues=evaluations --concurrency=4
celery -A app.core.celery_app:celery_app worker --queues=datasets --concurrency=2
celery -A app.core.celery_app:celery_app worker --queues=analytics --concurrency=2
celery -A app.core.celery_app:celery_app worker --queues=optimization --concurrency=2
celery -A app.core.celery_app:celery_app worker --queues=security --concurrency=3

# Optional: Start Celery Flower for monitoring
celery -A app.core.celery_app:celery_app flower --port=5555

# Frontend
cd frontend
npm run build
npm start
```

### **Environment Variables for Production**

- Set `ENVIRONMENT=production`
- Use strong `SECRET_KEY`
- Configure proper `CORS_ORIGINS`
- Set up SSL/TLS certificates
- Use managed database services (PostgreSQL, Redis)
- Configure production vector database (Qdrant Cloud or Pinecone)
- Set up monitoring and alerting
- Enable rate limiting and authentication

---

## 📁 **Project Structure**

```
evalia/
├── 📁 backend/                 # FastAPI Backend
│   ├── 📁 app/
│   │   ├── 📁 api/v1/         # API routes
│   │   │   ├── 📄 evaluations.py    # Evaluation endpoints
│   │   │   ├── 📄 datasets.py       # Dataset management
│   │   │   ├── 📄 models.py         # Model management
│   │   │   ├── 📄 analytics.py      # Analytics & insights
│   │   │   ├── 📄 prompts.py        # Prompt versioning & management
│   │   │   ├── 📄 security.py       # Security testing
│   │   │   └── 📄 compliance.py     # Compliance validation
│   │   ├── 📁 core/           # Core business logic
│   │   │   ├── 📁 evaluation/ # Evaluation engine
│   │   │   ├── 📁 llm/        # Multi-provider LLM integration
│   │   │   ├── 📁 datasets/   # Dataset management
│   │   │   ├── 📁 analytics/  # Performance analytics
│   │   │   ├── 📁 optimization/ # Prompt optimization engine
│   │   │   ├── 📁 versioning/ # Git-like prompt versioning
│   │   │   ├── 📁 security/   # Security testing engine
│   │   │   └── 📁 compliance/ # Compliance validation
│   │   ├── 📁 models/         # Pydantic models
│   │   │   ├── 📄 evaluation.py     # Evaluation models
│   │   │   ├── 📄 dataset.py        # Dataset models
│   │   │   ├── 📄 metrics.py        # Metrics models
│   │   │   ├── 📄 prompt.py         # Prompt & version models
│   │   │   ├── 📄 security.py       # Security models
│   │   │   └── 📄 compliance.py     # Compliance models
│   │   ├── 📁 services/       # Business services
│   │   │   ├── 📄 evaluation_service.py  # Evaluation logic
│   │   │   ├── 📄 dataset_service.py     # Dataset operations
│   │   │   ├── 📄 analytics_service.py   # Analytics processing
│   │   │   ├── 📄 optimization_service.py # Prompt optimization
│   │   │   ├── 📄 version_control_service.py # Prompt versioning
│   │   │   ├── 📄 security_service.py    # Security testing
│   │   │   └── 📄 compliance_service.py  # Compliance checks
│   │   ├── 📁 tasks/          # Celery background tasks
│   │   │   ├── 📄 evaluation_tasks.py    # Async evaluations
│   │   │   ├── 📄 dataset_tasks.py       # Dataset processing
│   │   │   ├── 📄 security_tasks.py      # Security scanning
│   │   │   └── 📄 optimization_tasks.py  # Prompt optimization
│   │   └── 📁 utils/          # Utilities
│   ├── 📁 docker/             # Docker configurations
│   ├── 📁 scripts/            # Deployment scripts
│   └── 📄 pyproject.toml      # Python dependencies (uv)
│
├── 📁 frontend/               # Next.js Dashboard
│   ├── 📁 src/
│   │   ├── 📁 app/            # Next.js App Router
│   │   │   ├── 📁 evaluations/      # Evaluation pages
│   │   │   ├── 📁 datasets/         # Dataset management
│   │   │   ├── 📁 analytics/        # Analytics dashboard
│   │   │   ├── 📁 models/           # Model comparison
│   │   │   ├── 📁 prompts/          # Prompt versioning & editor
│   │   │   ├── 📁 security/         # Security testing
│   │   │   └── 📁 compliance/       # Compliance dashboard
│   │   ├── 📁 components/     # React components
│   │   │   ├── 📁 ui/         # Base UI components
│   │   │   ├── 📁 charts/     # Data visualization
│   │   │   ├── 📁 evaluation/ # Evaluation components
│   │   │   ├── 📁 datasets/   # Dataset components
│   │   │   ├── 📁 prompts/    # Prompt editor & diff viewer
│   │   │   ├── 📁 optimization/ # Optimization feedback UI
│   │   │   ├── 📁 security/   # Security scan results
│   │   │   └── 📁 compliance/ # Compliance reports
│   │   ├── 📁 hooks/          # Custom React hooks
│   │   │   ├── 📄 use-evaluations.ts # Evaluation hooks
│   │   │   ├── 📄 use-analytics.ts   # Analytics hooks
│   │   │   ├── 📄 use-prompts.ts     # Prompt versioning hooks
│   │   │   ├── 📄 use-optimization.ts # Optimization hooks
│   │   │   └── 📄 use-security.ts    # Security testing hooks
│   │   ├── 📁 lib/            # Utilities & API client
│   │   └── 📁 types/          # TypeScript definitions
│   └── 📄 package.json       # Node.js dependencies
│
├── 📄 docker-compose.yml     # Development services
├── 📄 docker-compose.prod.yml # Production setup
└── 📄 README.md              # This file
```

---

## 🧪 **Development**

### **Running Tests**

```bash
# Backend tests
cd backend
uv run pytest

# Frontend tests
cd frontend
npm test
```

### **Code Quality**

```bash
# Backend linting & formatting
cd backend
uv run black .
uv run isort .
uv run ruff check .
uv run mypy .

# Frontend linting
cd frontend
npm run lint
npm run type-check
```

### **Database Migrations**

```bash
cd backend
uv run alembic revision --autogenerate -m "Description"
uv run alembic upgrade head
```

---

## 🔍 **Monitoring & Health Checks**

- **Health Check**: `GET /health`
- **Metrics**: `GET /metrics` (Prometheus format)
- **Evaluation Status**: `GET /api/v1/evaluations/status`
- **Model Health**: `GET /api/v1/models/health`
- **Vector DB Health**: `GET /api/v1/datasets/health`
- **Cost Tracking**: `GET /api/v1/analytics/costs`
- **Security Scan Status**: `GET /api/v1/security/status`
- **Compliance Score**: `GET /api/v1/compliance/score`
- **Vulnerability Dashboard**: `GET /api/v1/security/dashboard`
- **Optimization Service**: `GET /api/v1/optimization/health`
- **Version Control**: `GET /api/v1/prompts/stats`

---

## 🤝 **Contributing**

### 🔧 **Setup Pre-commit Hooks**

We use pre-commit hooks to ensure code quality. Set them up before making changes:

```bash
# Install and setup pre-commit hooks
./scripts/setup-pre-commit.sh

# Or manually:
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

The hooks will automatically check:

- **Python**: Black formatting, autoflake unused import removal, isort import sorting, flake8 linting, mypy type checking
- **Frontend**: Prettier formatting, ESLint linting
- **Security**: Secret detection, private key scanning
- **General**: Trailing whitespace, file endings, YAML/JSON validation

### 🚀 **Contribution Steps**

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. **Setup pre-commit hooks**: `./scripts/setup-pre-commit.sh`
4. Make your changes
5. Run tests: `uv run pytest && npm test`
6. Commit: `git commit -m 'Add amazing feature'` (pre-commit hooks will run automatically)
7. Push: `git push origin feature/amazing-feature`
8. Open a Pull Request

---

## 📄 **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## 🧱 **Tech Stack**

### **Backend**

- **[FastAPI](https://fastapi.tiangolo.com)** - Modern Python web framework
- **[PostgreSQL](https://postgresql.org)** - Primary database for evaluations and results
- **[Redis](https://redis.io)** - Caching and task queue
- **[Celery](https://celeryproject.org)** - Background task processing
- **[uv](https://github.com/astral-sh/uv)** - Ultra-fast Python package manager

### **Frontend**

- **[Next.js](https://nextjs.org)** - React framework with App Router
- **[TypeScript](https://typescriptlang.org)** - Type-safe JavaScript
- **[Tailwind CSS](https://tailwindcss.com)** - Utility-first CSS framework
- **[Recharts](https://recharts.org)** - Data visualization library

### **LLM Integration**

- **[OpenAI](https://openai.com)** - GPT-4 and GPT-3.5 models
- **[Anthropic](https://anthropic.com)** - Claude models
- **[Google AI](https://ai.google.dev)** - Gemini models
- **[Ollama](https://ollama.ai)** - Local open-source models

### **Vector Database**

- **[Qdrant](https://qdrant.tech)** - High-performance vector database
- **[Pinecone](https://pinecone.io)** - Managed vector database service

### **Deployment**

- **[Docker](https://docker.com)** - Containerization
- **[Docker Compose](https://docs.docker.com/compose)** - Multi-container orchestration

---

## 🙏 **Acknowledgments**

- **[OpenAI](https://openai.com)** - GPT models for evaluation and LLM-as-Judge
- **[Anthropic](https://anthropic.com)** - Claude models for nuanced evaluation
- **[FastAPI](https://fastapi.tiangolo.com)** - Modern Python web framework
- **[Next.js](https://nextjs.org)** - React framework for production
- **[uv](https://github.com/astral-sh/uv)** - Ultra-fast Python package manager
- **[Qdrant](https://qdrant.tech)** - Vector database for semantic search

---

## 📞 **Support**

- 📧 **Email**: <julien.wut@gmail.com>
- 🐛 **Issues**: [GitHub Issues](<https://github.com/Julien> W./evalia/issues)
- 📖 **Documentation**: [Project Wiki](<https://github.com/Julien> W./evalia/wiki)

---

## ✨ **Taglines**

> **"Evalia — Where prompts meet proof and security meets compliance."**

> **"Smarter prompt engineering through data, secure AI through validation."**

> **"Benchmark, optimize, and secure your LLM applications."**

> **"The evaluation and security layer for AI systems."**

> **"Build better AI. Ship it securely."**

---

<div align="center">

**Built with ❤️ for the AI engineering and security community**

[🧠 OpenAI](https://openai.com) • [🤖 Anthropic](https://anthropic.com) • [⚡ FastAPI](https://fastapi.tiangolo.com) • [⚛️ Next.js](https://nextjs.org) • [🔍 Qdrant](https://qdrant.tech)

**Transform your prompt engineering from trial-and-error into a measurable, scientific process — while ensuring your LLM applications meet enterprise security and compliance standards.**

</div>
