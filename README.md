# ğŸ§  Evalia â€” The Intelligence Layer for Prompt Evaluation

> **Where prompts meet proof.** Intelligent platform for evaluating, benchmarking, and improving large language model (LLM) prompts â€” automatically, continuously, and contextually.

Evalia empowers AI engineers, researchers, and product teams to understand how their prompts perform across tasks, domains, and models â€” with built-in analytics, adaptive datasets, and LLM-based evaluation.

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-00a393?style=flat&logo=fastapi)](https://fastapi.tiangolo.com)
[![Next.js](https://img.shields.io/badge/Next.js-14+-black?style=flat&logo=next.js)](https://nextjs.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5+-3178c6?style=flat&logo=typescript)](https://www.typescriptlang.org)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-412991?style=flat&logo=openai)](https://openai.com)
[![Anthropic](https://img.shields.io/badge/Anthropic-Claude-orange?style=flat)](https://anthropic.com)
[![Docker](https://img.shields.io/badge/Docker-ready-2496ed?style=flat&logo=docker)](https://www.docker.com)

---

## ğŸ’¡ **Why Evalia?**

Building great AI experiences requires **evidence-driven prompt design**. Evalia turns prompt engineering from trial-and-error into a measurable, scientific process â€” bridging the gap between intuition and performance.

---

## ğŸŒŸ **Core Features**

### ğŸ§© **Prompt Evaluation Engine**

- **Automated A/B Testing** of prompt variants across datasets, models, and criteria
- **Multi-Model Benchmarking** across GPT-4, Claude, Gemini, and more
- **Performance Metrics** including accuracy, tone, relevance, and custom criteria
- **Regression Detection** to catch prompt performance degradation

### ğŸ§  **LLM-as-Judge Scoring**

- **Advanced Model Evaluation** using GPT-4 or Claude for qualitative scoring
- **Reasoning & Explanations** for why outputs succeed or fail
- **Custom Evaluation Criteria** tailored to your specific use cases
- **Confidence Scoring** and uncertainty quantification

### ğŸ·ï¸ **Domain-Aware Datasets**

- **Business-Specific Datasets** for legal, e-commerce, healthcare, and more
- **Adaptive Dataset Evolution** that improves with feedback
- **Realistic Example Generation** using LLMs when datasets don't exist
- **Version Control** for dataset changes and improvements

### ğŸ”„ **Dynamic Dataset Builder**

- **On-the-Fly Generation** of realistic test cases using LLMs
- **Feedback Integration** to continuously improve dataset quality
- **Domain Expertise** injection for specialized use cases
- **Synthetic Data Pipeline** for privacy-sensitive applications

### ğŸ“Š **Insight Dashboard**

- **Performance Visualization** over time with interactive charts
- **Domain Comparison** to see which contexts perform best
- **Model Performance** side-by-side comparisons
- **Cost Analysis** and token usage optimization

### âš¡ **Continuous Evaluation Pipelines**

- **CI/CD Integration** for automatic prompt re-evaluation
- **Webhook Support** for real-time notifications
- **Scheduled Evaluations** for ongoing monitoring
- **Alert System** for performance degradation

### ğŸ” **Hybrid Metrics**

- **LLM-Based Evaluation** for nuanced quality assessment
- **Rule-Based Metrics** for objective measurements
- **Human-in-the-Loop** validation and feedback
- **Custom Scoring Functions** for specialized requirements

### ğŸ§¬ **Model & Domain Comparison**

- **Cross-Model Benchmarking** (OpenAI, Anthropic, Google, etc.)
- **Domain-Specific Performance** analysis
- **Cost-Performance Trade-offs** visualization
- **Latency vs Quality** optimization insights

---

## ğŸš€ **Use Cases**

### ğŸ¯ **Customer Support Optimization**

Evaluate customer-support prompts across different industries and optimize response quality, tone, and accuracy.

### ğŸ“Š **Content Generation Benchmarking**

Compare summarization, QA, and content generation prompts between models (GPT-4, Claude, Gemini) to find the best fit.

### ğŸ”¬ **Research & Development**

Fine-tuning validation, RAG system optimization, and prompt experimentation for AI labs and research teams.

### ğŸ¢ **Enterprise AI Tooling**

Internal tooling for prompt experimentation, A/B testing, and performance monitoring at scale.

---

## ğŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js       â”‚â”€â”€â”€â”€â”‚   FastAPI       â”‚â”€â”€â”€â”€â”‚  Evaluation     â”‚
â”‚   Dashboard     â”‚    â”‚   Backend       â”‚    â”‚   Engine        â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ React Charts  â”‚    â”‚ â€¢ Python 3.11+ â”‚    â”‚ â€¢ Multi-Model   â”‚
â”‚ â€¢ Tailwind CSS  â”‚    â”‚ â€¢ Async/Await   â”‚    â”‚ â€¢ LLM-as-Judge  â”‚
â”‚ â€¢ TypeScript    â”‚    â”‚ â€¢ Pydantic      â”‚    â”‚ â€¢ Metrics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis  â”‚ â”‚ Celery â”‚ â”‚ Qdrant â”‚ â”‚   Dataset   â”‚ â”‚PostgreSQLâ”‚
â”‚ Cache  â”‚ â”‚ Queue  â”‚ â”‚Vector  â”‚ â”‚   Storage   â”‚ â”‚ Database â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ **Quick Start**

### 1. **Prerequisites**

```bash
# Install uv (ultra-fast Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Node.js 18+
# https://nodejs.org/

# Install Docker & Docker Compose
# https://docs.docker.com/get-docker/
```

### 2. **Environment Setup**

```bash
# Copy environment files
cp backend/.env.template backend/.env
cp frontend/.env.template frontend/.env

# Set your API keys in backend/.env
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here  # Optional for Gemini

# Vector database for embeddings (choose one)
QDRANT_URL=http://localhost:6333
# OR
PINECONE_API_KEY=your_pinecone_key_here
```

### 3. **Start with Docker Compose** âš¡

```bash
# Start all services (recommended for first run)
docker-compose up -d

# Or use the development setup
docker-compose -f docker-compose.dev.yml up -d
```

### 4. **Manual Development Setup** ğŸ› ï¸

```bash
# Backend
cd backend
uv venv
source .venv/bin/activate  # or `.venv\Scripts\activate` on Windows
uv pip install -e .
uv pip list
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Frontend (in another terminal)
cd frontend
npm install
npm run dev
```

### 5. **Access Evalia** ğŸ‰

- **Dashboard**: <http://localhost:3000>
- **Backend API**: <http://localhost:8000>
- **API Docs**: <http://localhost:8000/docs>
- **Evaluation Results**: <http://localhost:3000/evaluations>

---

## ğŸ”§ **Evaluation Configuration**

### **Supported Models**

Evalia supports evaluation across multiple LLM providers:

```python
# OpenAI Models
"gpt-4"                    # Best for complex reasoning
"gpt-4-turbo"             # Faster GPT-4 variant
"gpt-3.5-turbo"           # Cost-effective option

# Anthropic Models
"claude-3-opus"           # Most capable Claude model
"claude-3-sonnet"         # Balanced performance
"claude-3-haiku"          # Fast and efficient

# Google Models
"gemini-pro"              # Google's flagship model
"gemini-pro-vision"       # Multimodal capabilities

# Open Source (via Ollama)
"llama2"                  # Meta's Llama 2
"mistral"                 # Mistral 7B
"codellama"               # Code-specialized model
```

### **Evaluation Types**

**Quality Metrics**

- **Accuracy**: Correctness of responses
- **Relevance**: How well responses address the prompt
- **Coherence**: Logical flow and consistency
- **Completeness**: Coverage of required information

**LLM-as-Judge Evaluation**

- **GPT-4 Judge**: Use GPT-4 to evaluate response quality
- **Claude Judge**: Anthropic's Claude for nuanced assessment
- **Custom Criteria**: Define domain-specific evaluation criteria
- **Confidence Scoring**: Uncertainty quantification

### **Dataset Configuration**

**Built-in Domains**

- **Customer Support**: Service inquiries and responses
- **Content Generation**: Blog posts, summaries, descriptions
- **Code Generation**: Programming tasks and solutions
- **Legal**: Contract analysis and legal reasoning
- **Healthcare**: Medical information and advice
- **E-commerce**: Product descriptions and recommendations

**Custom Datasets**

- **CSV Import**: Upload your own evaluation datasets
- **API Integration**: Connect to existing data sources
- **Synthetic Generation**: AI-generated test cases
- **Version Control**: Track dataset changes over time

---

## ğŸ“š **API Endpoints**

### **Prompt Evaluation**

```bash
# Create a new evaluation
POST /api/v1/evaluations
{
  "name": "Customer Support Prompt v2",
  "prompts": [
    {
      "id": "prompt_a",
      "content": "You are a helpful customer support agent..."
    },
    {
      "id": "prompt_b", 
      "content": "As a customer service representative..."
    }
  ],
  "dataset_id": "customer_support_v1",
  "models": ["gpt-4", "claude-3-sonnet"],
  "evaluation_criteria": ["accuracy", "tone", "helpfulness"]
}

# Get evaluation results
GET /api/v1/evaluations/{evaluation_id}/results

# List all evaluations
GET /api/v1/evaluations

# Compare prompt performance
GET /api/v1/evaluations/{evaluation_id}/compare
```

### **Dataset Management**

```bash
# Upload a new dataset
POST /api/v1/datasets
{
  "name": "Customer Support Q&A",
  "domain": "customer_support",
  "data": [
    {
      "input": "How do I return a product?",
      "expected_output": "You can return products within 30 days...",
      "metadata": {"category": "returns"}
    }
  ]
}

# Get dataset details
GET /api/v1/datasets/{dataset_id}

# Generate synthetic dataset
POST /api/v1/datasets/generate
{
  "domain": "e_commerce",
  "size": 100,
  "criteria": "product_descriptions"
}

# List available datasets
GET /api/v1/datasets
```

### **Model Management**

```bash
# Get available models
GET /api/v1/models

# Test model connectivity
POST /api/v1/models/{model_id}/test
{
  "prompt": "Hello, world!"
}

# Get model performance metrics
GET /api/v1/models/{model_id}/metrics

# Compare models
POST /api/v1/models/compare
{
  "models": ["gpt-4", "claude-3-sonnet"],
  "prompt": "Explain quantum computing",
  "evaluation_criteria": ["accuracy", "clarity"]
}
```

### **Analytics & Insights**

```bash
# Get evaluation analytics
GET /api/v1/analytics/evaluations/{evaluation_id}

# Performance trends over time
GET /api/v1/analytics/trends?timeframe=30d

# Cost analysis
GET /api/v1/analytics/costs?model=gpt-4&timeframe=7d

# Domain performance comparison
GET /api/v1/analytics/domains/compare
```

### **Background Tasks**

```bash
# Run evaluation asynchronously
POST /api/v1/tasks/evaluations
{
  "evaluation_id": "eval_123",
  "priority": "high"
}

# Get task status
GET /api/v1/tasks/{task_id}

# Schedule recurring evaluation
POST /api/v1/tasks/schedule
{
  "evaluation_id": "eval_123",
  "schedule": "0 9 * * 1"  # Every Monday at 9 AM
}
```

---

## ğŸ”§ **Configuration**

### **Backend Settings** (`backend/.env`)

```bash
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here

# Default Evaluation Settings
DEFAULT_EVALUATION_MODEL=gpt-4
DEFAULT_JUDGE_MODEL=gpt-4
ENABLE_COST_TRACKING=true
MAX_CONCURRENT_EVALUATIONS=5

# Vector Database (choose one)
VECTOR_DATABASE=qdrant
QDRANT_URL=http://localhost:6333
# OR
VECTOR_DATABASE=pinecone
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=gcp-starter

# Database & Cache
DATABASE_URL=postgresql://user:pass@localhost:5432/evalia
REDIS_URL=redis://localhost:6379/0

# Background Tasks
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/1
ENABLE_ASYNC_EVALUATIONS=true

# Monitoring & Analytics
ENABLE_METRICS=true
PROMETHEUS_PORT=9090
LOG_LEVEL=INFO
```

### **Frontend Settings** (`frontend/.env.local`)

```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_APP_NAME=Evalia
NEXT_PUBLIC_ENABLE_ANALYTICS=true
```

---

## ğŸš¢ **Deployment**

### **Using Docker** (Recommended)

```bash
# Production build
docker-compose -f docker-compose.prod.yml up -d

# Or use the deployment script
./backend/scripts/deploy.sh
```

### **Manual Deployment**

```bash
# Backend
cd backend
uv pip install -e .
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker

# Start Celery workers for evaluation tasks
celery -A app.core.celery_app:celery_app worker --queues=evaluations --concurrency=4
celery -A app.core.celery_app:celery_app worker --queues=datasets --concurrency=2
celery -A app.core.celery_app:celery_app worker --queues=analytics --concurrency=2

# Optional: Start Celery Flower for monitoring
celery -A app.core.celery_app:celery_app flower --port=5555

# Frontend
cd frontend
npm run build
npm start
```

### **Environment Variables for Production**

- Set `ENVIRONMENT=production`
- Use strong `SECRET_KEY`
- Configure proper `CORS_ORIGINS`
- Set up SSL/TLS certificates
- Use managed database services (PostgreSQL, Redis)
- Configure production vector database (Qdrant Cloud or Pinecone)
- Set up monitoring and alerting
- Enable rate limiting and authentication

---

## ğŸ“ **Project Structure**

```
evalia/
â”œâ”€â”€ ğŸ“ backend/                 # FastAPI Backend
â”‚   â”œâ”€â”€ ğŸ“ app/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api/v1/         # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ evaluations.py    # Evaluation endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ datasets.py       # Dataset management
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py         # Model management
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ analytics.py      # Analytics & insights
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core/           # Core business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ evaluation/ # Evaluation engine
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ llm/        # Multi-provider LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ datasets/   # Dataset management
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ analytics/  # Performance analytics
â”‚   â”‚   â”œâ”€â”€ ğŸ“ models/         # Pydantic models
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ evaluation.py     # Evaluation models
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataset.py        # Dataset models
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ metrics.py        # Metrics models
â”‚   â”‚   â”œâ”€â”€ ğŸ“ services/       # Business services
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ evaluation_service.py  # Evaluation logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataset_service.py     # Dataset operations
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ analytics_service.py   # Analytics processing
â”‚   â”‚   â”œâ”€â”€ ğŸ“ tasks/          # Celery background tasks
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ evaluation_tasks.py    # Async evaluations
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ dataset_tasks.py       # Dataset processing
â”‚   â”‚   â””â”€â”€ ğŸ“ utils/          # Utilities
â”‚   â”œâ”€â”€ ğŸ“ docker/             # Docker configurations
â”‚   â”œâ”€â”€ ğŸ“ scripts/            # Deployment scripts
â”‚   â””â”€â”€ ğŸ“„ pyproject.toml      # Python dependencies (uv)
â”‚
â”œâ”€â”€ ğŸ“ frontend/               # Next.js Dashboard
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ app/            # Next.js App Router
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ evaluations/      # Evaluation pages
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ datasets/         # Dataset management
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ analytics/        # Analytics dashboard
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ models/           # Model comparison
â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/     # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ ui/         # Base UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ charts/     # Data visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ evaluation/ # Evaluation components
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ datasets/   # Dataset components
â”‚   â”‚   â”œâ”€â”€ ğŸ“ hooks/          # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ use-evaluations.ts # Evaluation hooks
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ use-analytics.ts   # Analytics hooks
â”‚   â”‚   â”œâ”€â”€ ğŸ“ lib/            # Utilities & API client
â”‚   â”‚   â””â”€â”€ ğŸ“ types/          # TypeScript definitions
â”‚   â””â”€â”€ ğŸ“„ package.json       # Node.js dependencies
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml     # Development services
â”œâ”€â”€ ğŸ“„ docker-compose.prod.yml # Production setup
â””â”€â”€ ğŸ“„ README.md              # This file
```

---

## ğŸ§ª **Development**

### **Running Tests**

```bash
# Backend tests
cd backend
uv run pytest

# Frontend tests
cd frontend
npm test
```

### **Code Quality**

```bash
# Backend linting & formatting
cd backend
uv run black .
uv run isort .
uv run ruff check .
uv run mypy .

# Frontend linting
cd frontend
npm run lint
npm run type-check
```

### **Database Migrations**

```bash
cd backend
uv run alembic revision --autogenerate -m "Description"
uv run alembic upgrade head
```

---

## ğŸ” **Monitoring & Health Checks**

- **Health Check**: `GET /health`
- **Metrics**: `GET /metrics` (Prometheus format)
- **Evaluation Status**: `GET /api/v1/evaluations/status`
- **Model Health**: `GET /api/v1/models/health`
- **Vector DB Health**: `GET /api/v1/datasets/health`
- **Cost Tracking**: `GET /api/v1/analytics/costs`

---

## ğŸ¤ **Contributing**

### ğŸ”§ **Setup Pre-commit Hooks**

We use pre-commit hooks to ensure code quality. Set them up before making changes:

```bash
# Install and setup pre-commit hooks
./scripts/setup-pre-commit.sh

# Or manually:
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

The hooks will automatically check:

- **Python**: Black formatting, autoflake unused import removal, isort import sorting, flake8 linting, mypy type checking
- **Frontend**: Prettier formatting, ESLint linting
- **Security**: Secret detection, private key scanning
- **General**: Trailing whitespace, file endings, YAML/JSON validation

### ğŸš€ **Contribution Steps**

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. **Setup pre-commit hooks**: `./scripts/setup-pre-commit.sh`
4. Make your changes
5. Run tests: `uv run pytest && npm test`
6. Commit: `git commit -m 'Add amazing feature'` (pre-commit hooks will run automatically)
7. Push: `git push origin feature/amazing-feature`
8. Open a Pull Request

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ§± **Tech Stack**

### **Backend**

- **[FastAPI](https://fastapi.tiangolo.com)** - Modern Python web framework
- **[PostgreSQL](https://postgresql.org)** - Primary database for evaluations and results
- **[Redis](https://redis.io)** - Caching and task queue
- **[Celery](https://celeryproject.org)** - Background task processing
- **[uv](https://github.com/astral-sh/uv)** - Ultra-fast Python package manager

### **Frontend**

- **[Next.js](https://nextjs.org)** - React framework with App Router
- **[TypeScript](https://typescriptlang.org)** - Type-safe JavaScript
- **[Tailwind CSS](https://tailwindcss.com)** - Utility-first CSS framework
- **[Recharts](https://recharts.org)** - Data visualization library

### **LLM Integration**

- **[OpenAI](https://openai.com)** - GPT-4 and GPT-3.5 models
- **[Anthropic](https://anthropic.com)** - Claude models
- **[Google AI](https://ai.google.dev)** - Gemini models
- **[Ollama](https://ollama.ai)** - Local open-source models

### **Vector Database**

- **[Qdrant](https://qdrant.tech)** - High-performance vector database
- **[Pinecone](https://pinecone.io)** - Managed vector database service

### **Deployment**

- **[Docker](https://docker.com)** - Containerization
- **[Docker Compose](https://docs.docker.com/compose)** - Multi-container orchestration

---

## ğŸ™ **Acknowledgments**

- **[OpenAI](https://openai.com)** - GPT models for evaluation and LLM-as-Judge
- **[Anthropic](https://anthropic.com)** - Claude models for nuanced evaluation
- **[FastAPI](https://fastapi.tiangolo.com)** - Modern Python web framework
- **[Next.js](https://nextjs.org)** - React framework for production
- **[uv](https://github.com/astral-sh/uv)** - Ultra-fast Python package manager
- **[Qdrant](https://qdrant.tech)** - Vector database for semantic search

---

## ğŸ“ **Support**

- ğŸ“§ **Email**: <julien.wut@gmail.com>
- ğŸ› **Issues**: [GitHub Issues](<https://github.com/Julien> W./evalia/issues)
- ğŸ“– **Documentation**: [Project Wiki](<https://github.com/Julien> W./evalia/wiki)

---

## âœ¨ **Taglines**

> **"Evalia â€” Where prompts meet proof."**

> **"Smarter prompt engineering through data."**

> **"Benchmark, optimize, and evolve your prompts."**

> **"The evaluation layer for AI systems."**

---

<div align="center">

**Built with â¤ï¸ for the AI engineering community**

[ğŸ§  OpenAI](https://openai.com) â€¢ [ğŸ¤– Anthropic](https://anthropic.com) â€¢ [âš¡ FastAPI](https://fastapi.tiangolo.com) â€¢ [âš›ï¸ Next.js](https://nextjs.org) â€¢ [ğŸ” Qdrant](https://qdrant.tech)

**Transform your prompt engineering from trial-and-error into a measurable, scientific process.**

</div>
